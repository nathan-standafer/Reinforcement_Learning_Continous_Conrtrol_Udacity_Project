{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up initial environment\n",
    "The cell below instantiates the environment and sets some initial variables:\n",
    "\n",
    "- brain_name\n",
    "- action_size: the number of actions that can be performed in the environment\n",
    "- state_size: the number of values retured from the envionment to represent the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0909999979659915\n",
      "max episode length: 1001\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "counter = 0\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    counter += 1\n",
    "    #print('dones: {}'.format(dones))\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "print('max episode length: {}'.format(counter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define netowrks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "# actor - take in a state and output a distribution of actions\n",
    "class ActorModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 256)\n",
    "        self.out = torch.nn.Linear(256, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.out.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        batch_size = states.size(0)\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.tanh(self.out(x))\n",
    "        return x\n",
    "\n",
    "# critic - take in a state AND actions - outputs a state value function - V\n",
    "class CriticModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(CriticModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.fc2 = torch.nn.Linear(256+action_size, 256)\n",
    "        self.fc3 = torch.nn.Linear(256, 128)\n",
    "        self.out = torch.nn.Linear(128, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.out.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        batch_size = states.size(0)\n",
    "        xs = F.leaky_relu(self.fc1(states))\n",
    "        x = torch.cat((xs, actions), dim=1) #add in actions to the network\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class StepInfo:\n",
    "    def __init__(self, step_number, states, actions, rewards, next_states, dones):\n",
    "        self.step_number = step_number\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.next_states = next_states\n",
    "        self.dones = dones\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"step_number: {},  states: {},  actions: {},  rewards: {},  next_states: {}\".format(self.step_number, self.states, self.actions, self.rewards, self.next_states)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "def play_game(env, brain_name, replay_buffer, actor_model, add_noise=True):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations     \n",
    "    # get the current state (for each agent)\n",
    "    #random_seed = random.randint(0,1000)\n",
    "    random_seed = 2\n",
    "    noise = OUNoise(actor_model.action_size, random_seed)\n",
    "\n",
    "    #Play a game. Add to the replay_buffer\n",
    "    step_number = 0\n",
    "    total_rewards = 0.0\n",
    "    while True:\n",
    "        state_tensor = torch.from_numpy(states).float().cuda()\n",
    "\n",
    "        #actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actor_model.eval()\n",
    "        with torch.no_grad():\n",
    "            actions_tensor = actor_model(state_tensor)\n",
    "        actor_model.train()\n",
    "        actions_np = actions_tensor.detach().cpu().numpy()\n",
    "        #print(\"actions: {}\".format(actions_np.shape))\n",
    "        #actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions_np)[brain_name]         # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations          # get next state (for each agent)\n",
    "        rewards = env_info.rewards                          # get reward (for each agent)\n",
    "        #print(\"rewards: {}\".format(rewards))\n",
    "        total_rewards += np.sum(rewards)\n",
    "        dones = env_info.local_done                         # see if episode finished\n",
    "\n",
    "        #print(\"actions before noise: {}\".format(actions_np))\n",
    "        if add_noise:\n",
    "            actions_np += noise.sample()\n",
    "            np.clip(actions_np, -1, 1)\n",
    "        #print(\"actions after noise: {}\".format(actions_np))\n",
    "    \n",
    "        this_step_info = StepInfo(step_number, states, actions_np, rewards, next_states, dones)\n",
    "        replay_buffer.append(this_step_info)\n",
    "        step_number += 1\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    return total_rewards\n",
    "            \n",
    "def soft_update_target(local_model, target_model, tau):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process (A3C): \n",
    "# Feed current_state into actor network to get the action to take in that state\n",
    "#     Get next_state and rewards from the env\n",
    "\n",
    "# Feed next_state (s-prime) into critic network to get critic_next_state_reward\n",
    "#     Train the critic for current state using: current_reward + gamma * critic_next_state_reward\n",
    "\n",
    "# Calculate Advantage (A) given current_state and action:\n",
    "#     A(current_state, action) = reward + gamma(critic_next_state_reward) - critic_current_state_reward\n",
    "\n",
    "# Train the Actor using the Calculated Advantage (A) as a baseline\n",
    "\n",
    "# N-step bootstrapping???  Lambda (Generalized Advantage Estimate)???\n",
    "# no replay buffer in AC3\n",
    "# ************** not sure A3C will work with continuous action spaces\n",
    "\n",
    "# training process (DDPG) - continuous action spaces\n",
    "# Query the actor network to get believed best action (BBA)\n",
    "# The critic evaluates the actor's BBA Given state AND actor's action values as input.\n",
    "# DDPG uses a replay buffer\n",
    "# Need a regular and target network for the actor and critic networks.\n",
    "#  update targets using soft update strategy - e.g. slowly mix in updates.\n",
    "#  target is used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# instantiate objects that will can be re-used\n",
    "buffer_length = 5000\n",
    "replay_buffer = deque(maxlen=buffer_length)\n",
    "\n",
    "actor_model_local   = ActorModel(state_size, action_size).cuda()\n",
    "actor_model_target  = ActorModel(state_size, action_size).cuda()\n",
    "critic_model_local  = CriticModel(state_size, action_size).cuda()\n",
    "critic_model_target = CriticModel(state_size, action_size).cuda()\n",
    "\n",
    "lr_actor = .0001\n",
    "lr_critic = .001\n",
    "weight_decay = .01\n",
    "actor_optimizer = optim.Adam(actor_model_local.parameters(), lr=lr_actor)\n",
    "critic_optimizer = optim.Adam(critic_model_local.parameters(), lr=lr_critic, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - total_rewards: 0.030, critic_loss: 0.000002, actor_loss: -0.001000\n",
      "epoch: 0 - total_rewards: 0.230, critic_loss: 0.000003, actor_loss: -0.000979\n",
      "epoch: 0 - total_rewards: 0.280, critic_loss: 0.000004, actor_loss: -0.000997\n",
      "epoch: 0 - total_rewards: 0.090, critic_loss: 0.000003, actor_loss: -0.000980\n",
      "epoch: 0 - total_rewards: 0.220, critic_loss: 0.000002, actor_loss: -0.000950\n",
      "epoch: 1 - total_rewards: 0.190, critic_loss: 0.000003, actor_loss: -0.000951\n",
      "epoch: 1 - total_rewards: 0.240, critic_loss: 0.000004, actor_loss: -0.000986\n",
      "epoch: 1 - total_rewards: 0.070, critic_loss: 0.000003, actor_loss: -0.000964\n",
      "epoch: 1 - total_rewards: 0.070, critic_loss: 0.000002, actor_loss: -0.000931\n",
      "epoch: 1 - total_rewards: 0.090, critic_loss: 0.000002, actor_loss: -0.000885\n",
      "epoch: 2 - total_rewards: 0.280, critic_loss: 0.000003, actor_loss: -0.000884\n",
      "epoch: 2 - total_rewards: 0.230, critic_loss: 0.000003, actor_loss: -0.000885\n",
      "epoch: 2 - total_rewards: 0.390, critic_loss: 0.000004, actor_loss: -0.000929\n",
      "epoch: 2 - total_rewards: 0.000, critic_loss: 0.000003, actor_loss: -0.000929\n",
      "epoch: 2 - total_rewards: 0.060, critic_loss: 0.000003, actor_loss: -0.000926\n",
      "epoch: 3 - total_rewards: 0.000, critic_loss: 0.000001, actor_loss: -0.000867\n",
      "epoch: 3 - total_rewards: 0.140, critic_loss: 0.000003, actor_loss: -0.000850\n",
      "epoch: 3 - total_rewards: 0.190, critic_loss: 0.000002, actor_loss: -0.000801\n",
      "epoch: 3 - total_rewards: 0.200, critic_loss: 0.000002, actor_loss: -0.000781\n",
      "epoch: 3 - total_rewards: 0.200, critic_loss: 0.000003, actor_loss: -0.000772\n",
      "epoch: 4 - total_rewards: 0.260, critic_loss: 0.000004, actor_loss: -0.000805\n",
      "epoch: 4 - total_rewards: 0.340, critic_loss: 0.000005, actor_loss: -0.000879\n",
      "epoch: 4 - total_rewards: 0.290, critic_loss: 0.000005, actor_loss: -0.000960\n",
      "epoch: 4 - total_rewards: 0.280, critic_loss: 0.000005, actor_loss: -0.001031\n",
      "epoch: 4 - total_rewards: 0.090, critic_loss: 0.000004, actor_loss: -0.001063\n",
      "epoch: 5 - total_rewards: 0.000, critic_loss: 0.000003, actor_loss: -0.001050\n",
      "epoch: 5 - total_rewards: 0.090, critic_loss: 0.000003, actor_loss: -0.000999\n",
      "epoch: 5 - total_rewards: 0.000, critic_loss: 0.000002, actor_loss: -0.000929\n",
      "epoch: 5 - total_rewards: 0.000, critic_loss: 0.000001, actor_loss: -0.000825\n",
      "epoch: 5 - total_rewards: 0.250, critic_loss: 0.000001, actor_loss: -0.000749\n",
      "epoch: 6 - total_rewards: 0.260, critic_loss: 0.000003, actor_loss: -0.000742\n",
      "epoch: 6 - total_rewards: 0.340, critic_loss: 0.000004, actor_loss: -0.000781\n",
      "epoch: 6 - total_rewards: 0.300, critic_loss: 0.000004, actor_loss: -0.000835\n",
      "epoch: 6 - total_rewards: 0.230, critic_loss: 0.000005, actor_loss: -0.000922\n",
      "epoch: 6 - total_rewards: 0.000, critic_loss: 0.000004, actor_loss: -0.000949\n",
      "epoch: 7 - total_rewards: 0.220, critic_loss: 0.000004, actor_loss: -0.000967\n",
      "epoch: 7 - total_rewards: 0.180, critic_loss: 0.000004, actor_loss: -0.000976\n",
      "epoch: 7 - total_rewards: 0.100, critic_loss: 0.000003, actor_loss: -0.000968\n",
      "epoch: 7 - total_rewards: 0.170, critic_loss: 0.000002, actor_loss: -0.000917\n",
      "epoch: 7 - total_rewards: 0.300, critic_loss: 0.000004, actor_loss: -0.000927\n",
      "epoch: 8 - total_rewards: 0.440, critic_loss: 0.000005, actor_loss: -0.000999\n",
      "epoch: 8 - total_rewards: 0.190, critic_loss: 0.000005, actor_loss: -0.001047\n",
      "epoch: 8 - total_rewards: 0.300, critic_loss: 0.000005, actor_loss: -0.001096\n",
      "epoch: 8 - total_rewards: 0.070, critic_loss: 0.000004, actor_loss: -0.001109\n",
      "epoch: 8 - total_rewards: 0.090, critic_loss: 0.000004, actor_loss: -0.001119\n",
      "epoch: 9 - total_rewards: 0.400, critic_loss: 0.000004, actor_loss: -0.001117\n",
      "epoch: 9 - total_rewards: 0.290, critic_loss: 0.000006, actor_loss: -0.001154\n",
      "epoch: 9 - total_rewards: 0.080, critic_loss: 0.000003, actor_loss: -0.001140\n",
      "epoch: 9 - total_rewards: 0.300, critic_loss: 0.000003, actor_loss: -0.001118\n",
      "epoch: 9 - total_rewards: 0.480, critic_loss: 0.000007, actor_loss: -0.001192\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# train the model\n",
    "\n",
    "epochs = 10\n",
    "games_per_epoch = 5\n",
    "learning_iterations_per_game = 4000\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "n_steps = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for game in range(games_per_epoch):\n",
    "        #play a game\n",
    "        total_actor_loss = 0.0\n",
    "        total_critic_loss = 0.0\n",
    "        \n",
    "        # replay_buffer = deque(maxlen=buffer_length)  #see what happens if we only store 1 game\n",
    "        total_rewards = play_game(env, brain_name, replay_buffer, actor_model_local)\n",
    "    \n",
    "        #do some learning\n",
    "        for learning_iteration in range(learning_iterations_per_game):\n",
    "            rand_frame_index = random.randint(0,len(replay_buffer)-1-n_steps)\n",
    "            \n",
    "            #make sure we don't hit the end of a playthrough when calculating n-step bootstrapping\n",
    "            while True:\n",
    "                for n_step in range(0, n_steps+1):\n",
    "                    if np.any(replay_buffer[rand_frame_index+n_step].dones):\n",
    "                        rand_frame_index = random.randint(0,len(replay_buffer)-1-n_steps)\n",
    "                        continue\n",
    "                break\n",
    "            \n",
    "            this_replay_buffer = replay_buffer[rand_frame_index]\n",
    "            \n",
    "            # ---------------------------- update critic ---------------------------- #\n",
    "            # Get predicted next-state actions and Q values from target models\n",
    "            #next_states = this_replay_buffer.next_states\n",
    "            #next_states_tensor = torch.from_numpy(next_states).float().cuda()\n",
    "            #actions_next = actor_model_target(next_states_tensor)\n",
    "            #Q_targets_next = critic_model_target(next_states_tensor, actions_next)\n",
    "            #print(\"Q_targets_next: {}\".format(Q_targets_next))\n",
    "            \n",
    "            # Compute Q targets for current states (y_i)\n",
    "            # Use n-step bootstrapping to calculate better TD estimate\n",
    "            n_step_rewards = np.asarray(this_replay_buffer.rewards)\n",
    "            for n_step in range(1, n_steps):\n",
    "                n_step_index = rand_frame_index + n_step\n",
    "                n_step_replay_buffer = replay_buffer[n_step_index]\n",
    "                n_step_rewards += (gamma**n_step) * np.asarray(n_step_replay_buffer.rewards)\n",
    "            #get infinite_step value using model.\n",
    "            n_step_replay_buffer = replay_buffer[rand_frame_index + n_steps]\n",
    "            n_step_last_next_states_tensor = torch.from_numpy(n_step_replay_buffer.next_states).float().cuda()\n",
    "            n_step_last_actions_next = actor_model_target(n_step_last_next_states_tensor)\n",
    "            Q_targets_n_step_last = critic_model_target(n_step_last_next_states_tensor, n_step_last_actions_next)\n",
    "#             print(\"Q_targets_n_step_last.shape: {}\".format(Q_targets_n_step_last.shape))\n",
    "#             print(\"n_step_rewards.shape: {}\".format(n_step_rewards.shape))\n",
    "            \n",
    "            #rewards = np.asarray(this_replay_buffer.rewards)\n",
    "            #rewards_tensor = torch.from_numpy(rewards).float().cuda()\n",
    "            n_step_rewards_tensor = torch.from_numpy(n_step_rewards).float().cuda()\n",
    "            n_step_rewards_tensor = n_step_rewards_tensor.unsqueeze(dim=1)\n",
    "#            print(\"n_step_rewards_tensor: {}\".format(n_step_rewards_tensor))\n",
    "#             print(\"n_step_rewards_tensor.shape: {}\".format(n_step_rewards_tensor.shape))\n",
    "#             print(\"Q_targets_n_step_last: {}\".format(Q_targets_n_step_last))\n",
    "#             print(\"n_step_rewards_tensor: {}\".format(n_step_rewards_tensor))\n",
    "#             print(\"Q_targets_n_step_last + n_step_rewards_tensor: {}\".format(Q_targets_n_step_last + n_step_rewards_tensor))\n",
    "            \n",
    "            #Q_targets = rewards_tensor + (gamma * Q_targets_next)\n",
    "            Q_targets = n_step_rewards_tensor + ((gamma**n_steps) * Q_targets_n_step_last)\n",
    "#            print(\"Q_targets: {}\".format(Q_targets))\n",
    "                \n",
    "            # Compute critic loss\n",
    "            states = this_replay_buffer.states\n",
    "            states_tensor = torch.from_numpy(states).float().cuda()\n",
    "            actions = this_replay_buffer.actions\n",
    "            actions_tensor = torch.from_numpy(actions).float().cuda()\n",
    "\n",
    "            Q_expected = critic_model_local(states_tensor, actions_tensor)\n",
    "#             print(\"Q_expected: {}\".format(Q_expected))\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "            \n",
    "            # Minimize the critic loss\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            total_critic_loss += critic_loss.item()\n",
    "\n",
    "            # ---------------------------- update actor ---------------------------- #\n",
    "            # Compute actor loss\n",
    "            actions_pred = actor_model_local(states_tensor)\n",
    "            actor_loss = -critic_model_local(states_tensor, actions_pred).mean()\n",
    "\n",
    "            # Minimize the loss\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            total_actor_loss += actor_loss.item()\n",
    "\n",
    "            # ----------------------- update target networks ----------------------- #\n",
    "            #use very small Tau and update with every step\n",
    "            soft_update_target(critic_model_local, critic_model_target, tau)\n",
    "            soft_update_target(actor_model_local, actor_model_target, tau)\n",
    "            \n",
    "        print(\"epoch: {} - total_rewards: {:.3f}, critic_loss: {:.6f}, actor_loss: {:.6f}\".format(epoch, total_rewards, total_critic_loss/learning_iterations_per_game, total_actor_loss/learning_iterations_per_game))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rewards: 0.2899999935179949\n"
     ]
    }
   ],
   "source": [
    "#play a game without noise\n",
    "replay = []\n",
    "total_rewards = play_game(env, brain_name, replay, actor_model_local, add_noise=False)\n",
    "print(\"total rewards: {}\".format(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(actor_model_local.state_dict(),  \"actor_model_local.pt\")\n",
    "torch.save(actor_model_target.state_dict(), \"actor_model_target.pt\")\n",
    "torch.save(critic_model_local.state_dict(), \"critic_model_local.pt\")\n",
    "torch.save(critic_model_target.state_dict(),\"critic_model_target.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TheModelClass(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
