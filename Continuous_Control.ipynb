{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control - Project Submission\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up initial environment\n",
    "The cell below instantiates the environment and sets some initial variables:\n",
    "\n",
    "- brain_name\n",
    "- action_size: the number of actions that can be performed in the environment\n",
    "- state_size: the number of values retured from the envionment to represent the current state\n",
    "\n",
    "In this case, the envionment will run a single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726671e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define networks and helper methods.\n",
    "\n",
    "Define the Actor and Critic networks for the Deep Deterministic Policy Gradient (DDPG) networks.\n",
    " - Actor Model:  Receives the envionment state and outputs actions.  A basic fully connected network with ReLy activation on the two hidden layers and tahn activatio on teh output layer since the envornment receives actions values between -1 and 1\n",
    " - Critic Model:  Receives the envonment state and actions as input, outputs the predicted reward (state-value function).\n",
    " - StepInfo class: defines an step of the replay buffer used for training.\n",
    " - reset_game - resets the environment\n",
    " - soft_update_target - updates the weights using one model as a source and another as the target.  Weights are only updated fractionally, based upon the value of the input param ```tau```\n",
    " - env_step - Performs a step upon the environment.  The actions are chosen by submitting the given state values to the actor model. Epsilon represents the probability of choosing a random action instead of using the model.  Values are stored as a StepInfo object added to the replay buffer.\n",
    " - get_batch - get a tuple of values from the history buffer that is relevant for training.  Each member of the tuple is a list of random samples from the replay history with length equal to the desired batch size.  List indexes of the returen elements correspond to the same sample from the replay history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define netowrks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import random\n",
    "\n",
    "# actor - take in a state and output a distribution of actions\n",
    "class ActorModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 256)\n",
    "        self.out = torch.nn.Linear(256, action_size)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        batch_size = states.size(0)\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.tanh(self.out(x))\n",
    "        return x\n",
    "\n",
    "# critic - take in a state AND actions - outputs a state value function - V\n",
    "class CriticModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(CriticModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.fc2 = torch.nn.Linear(256+action_size, 256)\n",
    "        self.fc3 = torch.nn.Linear(256, 128)\n",
    "        self.out = torch.nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        batch_size = states.size(0)\n",
    "        xs = F.leaky_relu(self.fc1(states))\n",
    "        x = torch.cat((xs, actions), dim=1) #add in actions to the network\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class StepInfo:\n",
    "    def __init__(self, step_number, states, actions, rewards, next_states, dones):\n",
    "        self.step_number = step_number\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.next_states = next_states\n",
    "        self.dones = dones\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"step_number: {},  states: {},  actions: {},  rewards: {},  next_states: {}\".format(self.step_number, self.states, self.actions, self.rewards, self.next_states)\n",
    "\n",
    "def reset_game(in_env, brain_name):\n",
    "    # **important note** When training the environment, set `train_mode=True`\n",
    "    env_info = in_env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations\n",
    "    return states\n",
    "\n",
    "def env_step(in_env, brain_name, states, replay_buffer, actor_model, epsilon, add_noise=True):\n",
    "    #Play a game. Add to the replay_buffer\n",
    "    if (len(replay_buffer) > 0):\n",
    "        step_number = replay_buffer[-1].step_number + 1\n",
    "    else:\n",
    "        step_number = 0\n",
    "\n",
    "    state_tensor = torch.from_numpy(states).float().cuda()\n",
    "\n",
    "    rand_num = random.uniform(0, 1)\n",
    "    if rand_num > epsilon:\n",
    "        actor_model.eval()\n",
    "        with torch.no_grad():\n",
    "            actions_tensor = actor_model(state_tensor)\n",
    "        actor_model.train()\n",
    "        actions_np = actions_tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        actions_np = np.random.randn(num_agents, action_size)  # select an action (for each agent)\n",
    "\n",
    "    env_info = in_env.step(actions_np)[brain_name]         # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations          # get next state (for each agent)\n",
    "    rewards = env_info.rewards                          # get reward (for each agent)\n",
    "    dones = env_info.local_done                         # see if episode finished\n",
    "\n",
    "    this_step_info = StepInfo(step_number, states, actions_np, rewards, next_states, dones)\n",
    "    replay_buffer.append(this_step_info)\n",
    "\n",
    "    return next_states\n",
    "            \n",
    "def soft_update_target(local_model, target_model, tau):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "def getBatch(replay_buffer, batch_size):\n",
    "    return_states = np.zeros((batch_size, state_size))\n",
    "    return_actions = np.zeros((batch_size, action_size))\n",
    "    return_rewards = np.zeros((batch_size, 1))\n",
    "    return_next_states = np.zeros((batch_size, state_size))\n",
    "    return_next_actions = np.zeros((batch_size, action_size))\n",
    "    \n",
    "#     print(\"replay_buffer[0].states.shape: {}\".format(replay_buffer[0].states.shape))\n",
    "#     print(\"replay_buffer[0].rewards[0]: {}\".format(replay_buffer[0].rewards[0]))\n",
    "#     print(\"replay_buffer[0].actions.shape: {}\".format(replay_buffer[0].actions.shape))\n",
    "#     print(\"replay_buffer[0].next_states.shape: {}\".format(replay_buffer[0].next_states.shape))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rand_frame_index = random.randint(0,len(replay_buffer)-2)\n",
    "        return_states[i] = replay_buffer[rand_frame_index].states[0]\n",
    "        return_actions[i] = replay_buffer[rand_frame_index].actions[0]\n",
    "        return_rewards[i] = replay_buffer[rand_frame_index].rewards[0]\n",
    "        return_next_states[i] = replay_buffer[rand_frame_index].next_states[0]\n",
    "        return_next_actions[i] = replay_buffer[rand_frame_index+1].actions[0]\n",
    "        #### TODO - make sure \"next\" actions don't roll over onto the next  playthrough.\n",
    "        \n",
    "    return return_states, return_actions, return_rewards, return_next_states, return_next_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Objects\n",
    "\n",
    "Objects in the next cell are used in the training loop.  These are stored in a separate \n",
    "cell to allow the trainng loop to ran multiple times without resetting the objects that \n",
    "need to be continuously updated.\n",
    "\n",
    "Relevant metadata values were chosen through a painful amound of trial-and-error.\n",
    " - buffer_length 100000 seems like big number, but seems to work \n",
    " - lr_actor / lr_actor larger learning rates proved unstable.\n",
    "  - weight_decay - a recommendation from a friend doing the same project.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# instantiate objects that will can be re-used\n",
    "buffer_length = 100000\n",
    "replay_buffer = deque(maxlen=buffer_length)\n",
    "\n",
    "actor_model_local   = ActorModel(state_size, action_size).cuda()\n",
    "actor_model_target  = ActorModel(state_size, action_size).cuda()\n",
    "critic_model_local  = CriticModel(state_size, action_size).cuda()\n",
    "critic_model_target = CriticModel(state_size, action_size).cuda()\n",
    "\n",
    "if True:  # set to false if running for the first time or fresh models are desired.\n",
    "    actor_model_local.load_state_dict(torch.load(\"actor_model_local.pt\"))\n",
    "    actor_model_target.load_state_dict(torch.load(\"actor_model_target.pt\"))\n",
    "    critic_model_local.load_state_dict(torch.load(\"critic_model_local.pt\"))\n",
    "    critic_model_target.load_state_dict(torch.load(\"critic_model_target.pt\"))\n",
    "\n",
    "lr_actor = .0002\n",
    "lr_critic = .0002\n",
    "weight_decay = 0.0\n",
    "actor_optimizer = optim.Adam(actor_model_local.parameters(), lr=lr_actor)\n",
    "critic_optimizer = optim.Adam(critic_model_local.parameters(), lr=lr_critic, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    " - Perform training.  \n",
    " - Perform a step on the environment, saving data to the replay buffer.\n",
    " - After each step is performed:\n",
    "   - Grab a batch of random step data from the replay history.  \n",
    "   - Train the local critic and actor models.\n",
    "   - Perform soft updates on teh target critic and actor models.\n",
    " - Save the models when done.\n",
    " \n",
    "This was ran multiple times.  The printout below is a sample after the model was already trained for about 200 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - epsilon: 0.050, total_rewards: 42.420, critic_loss: 0.00156044, actor_loss: -3.061823\n",
      "epoch: 1 - epsilon: 0.030, total_rewards: 57.680, critic_loss: 0.00168753, actor_loss: -3.019857\n",
      "epoch: 2 - epsilon: 0.010, total_rewards: 51.430, critic_loss: 0.00189521, actor_loss: -3.045541\n",
      "epoch: 3 - epsilon: 0.000, total_rewards: 46.930, critic_loss: 0.00184486, actor_loss: -3.055374\n",
      "epoch: 4 - epsilon: 0.000, total_rewards: 65.500, critic_loss: 0.00196355, actor_loss: -3.084221\n",
      "epoch: 5 - epsilon: 0.000, total_rewards: 66.210, critic_loss: 0.00223274, actor_loss: -3.121827\n",
      "epoch: 6 - epsilon: 0.000, total_rewards: 57.790, critic_loss: 0.00247114, actor_loss: -3.147329\n",
      "epoch: 7 - epsilon: 0.000, total_rewards: 48.800, critic_loss: 0.00250819, actor_loss: -3.118186\n",
      "epoch: 8 - epsilon: 0.000, total_rewards: 64.550, critic_loss: 0.00245082, actor_loss: -3.132777\n",
      "epoch: 9 - epsilon: 0.000, total_rewards: 47.650, critic_loss: 0.00268918, actor_loss: -3.132944\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "epochs = 10 # 5\n",
    "steps_per_epoch = 2000 \n",
    "learning_iterations_per_step = 1 \n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "batch_size = 128\n",
    "epsilon_decay_batches = 50\n",
    "epsilon_max = 0.05\n",
    "epsilon_min = 0.0\n",
    "current_state = reset_game(env, brain_name)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_actor_loss = 0.0\n",
    "    total_critic_loss = 0.0    \n",
    "    total_rewards = 0\n",
    "    epsilon = max(epsilon_min, epsilon_max-epoch/epsilon_decay_batches)\n",
    "    \n",
    "    for game_step in range(steps_per_epoch):\n",
    "        #play a game\n",
    "        #total_rewards = play_game(env, brain_name, replay_buffer, actor_model_local)\n",
    "        current_state = env_step(env, brain_name, current_state, replay_buffer, actor_model_local, epsilon, add_noise=True)\n",
    "        total_rewards += np.sum(replay_buffer[-1].rewards)\n",
    "        \n",
    "        #if the game is done, reset and continue\n",
    "        if np.any(replay_buffer[-1].dones):\n",
    "            # new game\n",
    "            current_state = reset_game(env, brain_name)\n",
    "            current_state = env_step(env, brain_name, current_state, replay_buffer, actor_model_local, epsilon, add_noise=True)\n",
    "            total_rewards += np.sum(replay_buffer[-1].rewards)\n",
    "        \n",
    "        if len(replay_buffer) < 10000:\n",
    "            continue  \n",
    "    \n",
    "        #do some learning\n",
    "        for learning_iteration in range(learning_iterations_per_step):\n",
    "            states, actions, rewards, next_states, next_actions = getBatch(replay_buffer, batch_size)\n",
    "\n",
    "            # convert to tensors for input into the models.\n",
    "            rewards_tensor = torch.from_numpy(rewards).float().cuda()\n",
    "            next_actions_tensor = torch.from_numpy(next_actions).float().cuda()\n",
    "            next_states_tensor = torch.from_numpy(next_states).float().cuda()\n",
    "            states_tensor = torch.from_numpy(states).float().cuda()\n",
    "            actions_tensor = torch.from_numpy(actions).float().cuda()\n",
    "            \n",
    "            # ---------------------------- update critic ---------------------------- #\n",
    "            # Get predicted next-state actions and Q values from target models\n",
    "            \n",
    "            # Compute Q targets for current states (y_i)\n",
    "            Q_targets_next = critic_model_target(next_states_tensor, next_actions_tensor)\n",
    "            Q_targets = rewards_tensor + (gamma * Q_targets_next)\n",
    "                \n",
    "            # Compute critic loss\n",
    "            Q_expected = critic_model_local(states_tensor, actions_tensor)\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "            \n",
    "            # Minimize the critic loss\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(critic_model_local.parameters(), 1)\n",
    "            critic_optimizer.step()\n",
    "            total_critic_loss += critic_loss.item()\n",
    "\n",
    "            # ---------------------------- update actor ---------------------------- #\n",
    "            # Compute actor loss\n",
    "            actions_pred = actor_model_local(states_tensor)\n",
    "            actor_loss = -critic_model_local(states_tensor, actions_pred).mean()\n",
    "\n",
    "            # Minimize the actor loss\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            total_actor_loss += actor_loss.item()\n",
    "\n",
    "            # ----------------------- update target networks ----------------------- #\n",
    "            #use very small Tau and update with every step\n",
    "            soft_update_target(critic_model_local, critic_model_target, tau)\n",
    "            soft_update_target(actor_model_local, actor_model_target, tau)\n",
    "            \n",
    "    print(\"epoch: {} - epsilon: {:.3f}, total_rewards: {:.3f}, critic_loss: {:.8f}, actor_loss: {:.6f}\".format(epoch, epsilon, total_rewards, total_critic_loss/learning_iterations_per_step/steps_per_epoch, total_actor_loss/learning_iterations_per_step/steps_per_epoch))\n",
    "    \n",
    "torch.save(actor_model_local.state_dict(),  \"actor_model_local.pt\")\n",
    "torch.save(actor_model_target.state_dict(), \"actor_model_target.pt\")\n",
    "torch.save(critic_model_local.state_dict(), \"critic_model_local.pt\")\n",
    "torch.save(critic_model_target.state_dict(),\"critic_model_target.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play a series of games to see how the model performs.  \n",
    "\n",
    "The score is printed after each game is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 1, total_rewards: 37.760\n",
      "game 2, total_rewards: 39.120\n",
      "game 3, total_rewards: 35.020\n",
      "game 4, total_rewards: 37.940\n",
      "game 5, total_rewards: 34.610\n",
      "game 6, total_rewards: 39.290\n",
      "game 7, total_rewards: 38.300\n",
      "game 8, total_rewards: 38.490\n",
      "game 9, total_rewards: 38.270\n",
      "game 10, total_rewards: 34.310\n",
      "game 11, total_rewards: 34.340\n",
      "game 12, total_rewards: 32.500\n",
      "game 13, total_rewards: 13.660\n",
      "game 14, total_rewards: 31.010\n",
      "game 15, total_rewards: 39.040\n",
      "game 16, total_rewards: 36.240\n",
      "game 17, total_rewards: 38.430\n",
      "game 18, total_rewards: 27.960\n",
      "game 19, total_rewards: 36.820\n",
      "game 20, total_rewards: 37.040\n",
      "game 21, total_rewards: 26.520\n",
      "game 22, total_rewards: 37.400\n",
      "game 23, total_rewards: 37.150\n",
      "game 24, total_rewards: 37.760\n",
      "game 25, total_rewards: 35.870\n",
      "game 26, total_rewards: 36.480\n",
      "game 27, total_rewards: 33.940\n",
      "game 28, total_rewards: 37.030\n",
      "game 29, total_rewards: 33.470\n",
      "game 30, total_rewards: 31.530\n",
      "game 31, total_rewards: 32.720\n",
      "game 32, total_rewards: 37.410\n",
      "game 33, total_rewards: 19.270\n",
      "game 34, total_rewards: 39.540\n",
      "game 35, total_rewards: 36.630\n",
      "game 36, total_rewards: 34.390\n",
      "game 37, total_rewards: 31.690\n",
      "game 38, total_rewards: 36.910\n",
      "game 39, total_rewards: 36.420\n",
      "game 40, total_rewards: 30.700\n",
      "game 41, total_rewards: 36.730\n",
      "game 42, total_rewards: 30.160\n",
      "game 43, total_rewards: 37.110\n",
      "game 44, total_rewards: 37.650\n",
      "game 45, total_rewards: 34.690\n",
      "game 46, total_rewards: 31.600\n",
      "game 47, total_rewards: 38.410\n",
      "game 48, total_rewards: 34.730\n",
      "game 49, total_rewards: 35.470\n",
      "game 50, total_rewards: 39.200\n",
      "game 51, total_rewards: 35.510\n",
      "game 52, total_rewards: 35.090\n",
      "game 53, total_rewards: 34.730\n",
      "game 54, total_rewards: 33.820\n",
      "game 55, total_rewards: 37.280\n",
      "game 56, total_rewards: 36.420\n",
      "game 57, total_rewards: 36.500\n",
      "game 58, total_rewards: 32.930\n",
      "game 59, total_rewards: 30.730\n",
      "game 60, total_rewards: 36.290\n",
      "game 61, total_rewards: 33.540\n",
      "game 62, total_rewards: 38.930\n",
      "game 63, total_rewards: 31.990\n",
      "game 64, total_rewards: 26.490\n",
      "game 65, total_rewards: 34.380\n",
      "game 66, total_rewards: 38.800\n",
      "game 67, total_rewards: 31.160\n",
      "game 68, total_rewards: 32.620\n",
      "game 69, total_rewards: 39.210\n",
      "game 70, total_rewards: 39.600\n",
      "game 71, total_rewards: 35.800\n",
      "game 72, total_rewards: 33.510\n",
      "game 73, total_rewards: 33.700\n",
      "game 74, total_rewards: 33.220\n",
      "game 75, total_rewards: 33.110\n",
      "game 76, total_rewards: 37.610\n",
      "game 77, total_rewards: 34.820\n",
      "game 78, total_rewards: 37.670\n",
      "game 79, total_rewards: 28.210\n",
      "game 80, total_rewards: 33.830\n",
      "game 81, total_rewards: 32.140\n",
      "game 82, total_rewards: 33.140\n",
      "game 83, total_rewards: 37.750\n",
      "game 84, total_rewards: 36.490\n",
      "game 85, total_rewards: 24.890\n",
      "game 86, total_rewards: 35.040\n",
      "game 87, total_rewards: 32.770\n",
      "game 88, total_rewards: 26.230\n",
      "game 89, total_rewards: 39.310\n",
      "game 90, total_rewards: 36.910\n",
      "game 91, total_rewards: 32.100\n",
      "game 92, total_rewards: 39.450\n",
      "game 93, total_rewards: 36.200\n",
      "game 94, total_rewards: 30.070\n",
      "game 95, total_rewards: 34.650\n",
      "game 96, total_rewards: 38.700\n",
      "game 97, total_rewards: 30.320\n",
      "game 98, total_rewards: 34.480\n",
      "game 99, total_rewards: 32.010\n",
      "game 100, total_rewards: 34.370\n",
      "Average reward over 100 games: 34.532\n"
     ]
    }
   ],
   "source": [
    "actor_model_local.load_state_dict(torch.load(\"actor_model_local.pt\"))\n",
    "actor_model_target.load_state_dict(torch.load(\"actor_model_target.pt\"))\n",
    "critic_model_local.load_state_dict(torch.load(\"critic_model_local.pt\"))\n",
    "critic_model_target.load_state_dict(torch.load(\"critic_model_target.pt\"))\n",
    "    \n",
    "epsilon = 0.0\n",
    "total_game_count = 100\n",
    "total_rewards_over_all_games = 0\n",
    "\n",
    "for game_count in range(total_game_count):\n",
    "    total_rewards = 0.0\n",
    "    current_state = reset_game(env, brain_name)\n",
    "    replay = []\n",
    "    \n",
    "    for i in range(2000):\n",
    "        current_state = env_step(env, brain_name, current_state, replay, actor_model_local, epsilon, add_noise=True)\n",
    "        total_rewards += np.sum(replay[-1].rewards)\n",
    "        total_rewards_over_all_games += np.sum(replay[-1].rewards)\n",
    "        if np.any(replay[-1].dones):\n",
    "            break\n",
    "        \n",
    "    print(\"game {}, total_rewards: {:.3f}\".format(game_count+1, total_rewards))\n",
    "    \n",
    "print(\"Average reward over {} games: {:.3f}\".format(total_game_count, total_rewards_over_all_games/total_game_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
