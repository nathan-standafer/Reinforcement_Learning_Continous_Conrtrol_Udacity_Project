{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up initial environment\n",
    "The cell below instantiates the environment and sets some initial variables:\n",
    "\n",
    "- brain_name\n",
    "- action_size: the number of actions that can be performed in the environment\n",
    "- state_size: the number of values retured from the envionment to represent the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.12299999725073577\n",
      "max episode length: 1001\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "counter = 0\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    counter += 1\n",
    "    #print('dones: {}'.format(dones))\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "print('max episode length: {}'.format(counter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define netowrks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "# actor - take in a state and output a distribution of actions\n",
    "class ActorModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 256)\n",
    "        self.out = torch.nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, states):\n",
    "        batch_size = states.size(0)\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.tanh(self.out(x))\n",
    "        return x\n",
    "\n",
    "# critic - take in a state AND actions - outputs a state value function - V\n",
    "class CriticModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(CriticModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.fc2 = torch.nn.Linear(256+action_size, 256)\n",
    "        self.out = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        batch_size = states.size(0)\n",
    "        xs = F.relu(self.fc1(states))\n",
    "        x = torch.cat((xs, actions), dim=1) #add in actions to the network\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class StepInfo:\n",
    "    def __init__(self, step_number, states, actions, rewards, next_states):\n",
    "        self.step_number = step_number\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.next_states = next_states\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"step_number: {},  states: {},  actions: {},  rewards: {},  next_states: {}\".format(self.step_number, self.states, self.actions, self.rewards, self.next_states)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "def play_game(env, brain_name, replay_buffer, actor_model, add_noise=True):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations     \n",
    "    # get the current state (for each agent)\n",
    "    random_seed = 1\n",
    "    noise = OUNoise(actor_model.action_size, random_seed)\n",
    "\n",
    "    #Play a game. Add to the replay_buffer\n",
    "    step_number = 0\n",
    "    total_rewards = 0.0\n",
    "    while True:\n",
    "        state_tensor = torch.from_numpy(states).float().cuda()\n",
    "\n",
    "        #actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions_tensor = actor_model_local(state_tensor)\n",
    "        actions_np = actions_tensor.detach().cpu().numpy()\n",
    "        #print(\"actions: {}\".format(actions_np))\n",
    "        #actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions_np)[brain_name]         # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations          # get next state (for each agent)\n",
    "        rewards = env_info.rewards                          # get reward (for each agent)\n",
    "        total_rewards += np.sum(rewards)\n",
    "        dones = env_info.local_done                         # see if episode finished\n",
    "\n",
    "        if add_noise:\n",
    "            actions_np += noise.sample()\n",
    "            np.clip(actions_np, -1, 1)\n",
    "    \n",
    "        this_step_info = StepInfo(step_number, states, actions_np, rewards, next_states)\n",
    "        replay_buffer.append(this_step_info)\n",
    "        step_number += 1\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    return total_rewards\n",
    "            \n",
    "def soft_update_target(local_model, target_model, tau):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process (A3C): \n",
    "# Feed current_state into actor network to get the action to take in that state\n",
    "#     Get next_state and rewards from the env\n",
    "\n",
    "# Feed next_state (s-prime) into critic network to get critic_next_state_reward\n",
    "#     Train the critic for current state using: current_reward + gamma * critic_next_state_reward\n",
    "\n",
    "# Calculate Advantage (A) given current_state and action:\n",
    "#     A(current_state, action) = reward + gamma(critic_next_state_reward) - critic_current_state_reward\n",
    "\n",
    "# Train the Actor using the Calculated Advantage (A) as a baseline\n",
    "\n",
    "# N-step bootstrapping???  Lambda (Generalized Advantage Estimate)???\n",
    "# no replay buffer in AC3\n",
    "# ************** not sure A3C will work with continuous action spaces\n",
    "\n",
    "# training process (DDPG) - continuous action spaces\n",
    "# Query the actor network to get believed best action (BBA)\n",
    "# The critic evaluates the actor's BBA Given state AND actor's action values as input.\n",
    "# DDPG uses a replay buffer\n",
    "# Need a regular and target network for the actor and critic networks.\n",
    "#  update targets using soft update strategy - e.g. slowly mix in updates.\n",
    "#  target is used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# instantiate objects that will can be re-used\n",
    "replay_buffer = deque(maxlen=1001)\n",
    "\n",
    "actor_model_local   = ActorModel(state_size, action_size).cuda()\n",
    "actor_model_target  = ActorModel(state_size, action_size).cuda()\n",
    "critic_model_local  = CriticModel(state_size, action_size).cuda()\n",
    "critic_model_target = CriticModel(state_size, action_size).cuda()\n",
    "\n",
    "lr = .001\n",
    "actor_optimizer = optim.Adam(actor_model_local.parameters(), lr=lr)\n",
    "critic_optimizer = optim.Adam(critic_model_local.parameters(), lr=lr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - total_rewards: 10.300, critic_loss: 0.00715, actor_loss: 0.240\n",
      "epoch: 0 - total_rewards: 10.380, critic_loss: 0.00618, actor_loss: 0.257\n",
      "epoch: 0 - total_rewards: 0.500, critic_loss: 0.00339, actor_loss: 0.400\n",
      "epoch: 0 - total_rewards: 1.850, critic_loss: 0.00293, actor_loss: 0.369\n",
      "epoch: 0 - total_rewards: 1.130, critic_loss: 0.00189, actor_loss: 0.317\n",
      "epoch: 1 - total_rewards: 1.960, critic_loss: 0.00176, actor_loss: 0.196\n",
      "epoch: 1 - total_rewards: 16.220, critic_loss: 0.00424, actor_loss: 0.109\n",
      "epoch: 1 - total_rewards: 15.330, critic_loss: 0.00303, actor_loss: 0.127\n",
      "epoch: 1 - total_rewards: 15.140, critic_loss: 0.00203, actor_loss: 0.101\n",
      "epoch: 1 - total_rewards: 17.150, critic_loss: 0.00179, actor_loss: 0.121\n",
      "epoch: 2 - total_rewards: 20.060, critic_loss: 0.00161, actor_loss: 0.077\n",
      "epoch: 2 - total_rewards: 17.940, critic_loss: 0.00121, actor_loss: 0.108\n",
      "epoch: 2 - total_rewards: 8.020, critic_loss: 0.00120, actor_loss: 0.095\n",
      "epoch: 2 - total_rewards: 16.840, critic_loss: 0.00106, actor_loss: 0.083\n",
      "epoch: 2 - total_rewards: 20.540, critic_loss: 0.00097, actor_loss: 0.085\n",
      "epoch: 3 - total_rewards: 16.970, critic_loss: 0.00078, actor_loss: 0.072\n",
      "epoch: 3 - total_rewards: 15.510, critic_loss: 0.00069, actor_loss: 0.064\n",
      "epoch: 3 - total_rewards: 11.930, critic_loss: 0.00060, actor_loss: 0.064\n",
      "epoch: 3 - total_rewards: 15.930, critic_loss: 0.00055, actor_loss: 0.059\n",
      "epoch: 3 - total_rewards: 13.430, critic_loss: 0.00067, actor_loss: 0.053\n",
      "epoch: 4 - total_rewards: 13.640, critic_loss: 0.00045, actor_loss: 0.052\n",
      "epoch: 4 - total_rewards: 6.970, critic_loss: 0.00054, actor_loss: 0.054\n",
      "epoch: 4 - total_rewards: 11.560, critic_loss: 0.00041, actor_loss: 0.057\n",
      "epoch: 4 - total_rewards: 9.580, critic_loss: 0.00032, actor_loss: 0.055\n",
      "epoch: 4 - total_rewards: 11.730, critic_loss: 0.00026, actor_loss: 0.055\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# train the model\n",
    "\n",
    "epochs = 5\n",
    "games_per_epoch = 5\n",
    "learning_iterations_per_game = 1000\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for game in range(games_per_epoch):\n",
    "        #play a game\n",
    "        total_actor_loss = 0.0\n",
    "        total_critic_loss = 0.0\n",
    "        total_rewards = play_game(env, brain_name, replay_buffer, actor_model_local)\n",
    "        \n",
    "        #do some learning\n",
    "        for learning_iteration in range(learning_iterations_per_game):\n",
    "            rand_frame_index = random.randint(0,len(replay_buffer)-1)\n",
    "            this_replay_buffer = replay_buffer[rand_frame_index]\n",
    "            \n",
    "            # ---------------------------- update critic ---------------------------- #\n",
    "            # Get predicted next-state actions and Q values from target models\n",
    "            next_states = this_replay_buffer.next_states\n",
    "            next_states_tensor = torch.from_numpy(next_states).float().cuda()\n",
    "            actions_next = actor_model_target(next_states_tensor)\n",
    "            Q_targets_next = critic_model_target(next_states_tensor, actions_next)\n",
    "            #print(\"Q_targets_next: {}\".format(Q_targets_next))\n",
    "            \n",
    "            # Compute Q targets for current states (y_i)\n",
    "            rewards = np.asarray(this_replay_buffer.rewards)\n",
    "            rewards_tensor = torch.from_numpy(rewards).float().cuda()\n",
    "            Q_targets = rewards_tensor + (gamma * Q_targets_next)\n",
    "            #print(\"Q_targets: {}\".format(Q_targets))\n",
    "                \n",
    "            # Compute critic loss\n",
    "            states = this_replay_buffer.states\n",
    "            states_tensor = torch.from_numpy(states).float().cuda()\n",
    "            actions = this_replay_buffer.actions\n",
    "            actions_tensor = torch.from_numpy(actions).float().cuda()\n",
    "            Q_expected = critic_model_local(states_tensor, actions_tensor)\n",
    "            #print(\"Q_expected: {}\".format(Q_expected))\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "            \n",
    "            # Minimize the critic loss\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            total_critic_loss += critic_loss.item()\n",
    "\n",
    "            # ---------------------------- update actor ---------------------------- #\n",
    "            # Compute actor loss\n",
    "            actions_pred = actor_model_local(states_tensor)\n",
    "            actor_loss = -critic_model_local(states_tensor, actions_pred).mean()\n",
    "\n",
    "            # Minimize the loss\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            total_actor_loss += actor_loss.item()\n",
    "\n",
    "        print(\"epoch: {} - total_rewards: {:.3f}, critic_loss: {:.5f}, actor_loss: {:.3f}\".format(epoch, total_rewards, total_critic_loss/learning_iterations_per_game, total_actor_loss/learning_iterations_per_game))\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        soft_update_target(critic_model_local, critic_model_target, tau)\n",
    "        soft_update_target(actor_model_local, actor_model_target, tau)\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_number: 10,  states: [[ -6.25175476e-01  -3.83656120e+00  -9.47231293e-01   9.89733815e-01\n",
      "   -7.74296001e-02  -9.36788600e-03  -1.19766712e-01   5.88203520e-02\n",
      "    8.09767917e-02  -3.31189007e-01  -1.35536969e+00   2.53667235e-01\n",
      "   -1.78696021e-01  -9.46197510e-02  -8.89178371e+00   1.54876709e-01\n",
      "    9.20951068e-01   1.35189489e-01  -1.20881870e-01   3.44906658e-01\n",
      "   -9.56944346e-01   8.38525414e-01   3.88048559e-01   1.13146555e+00\n",
      "    3.52377415e+00   1.65761459e+00   7.75623322e+00  -1.00000000e+00\n",
      "    1.95980072e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -1.29508138e-01]\n",
      " [ -5.81001282e-01  -3.80603123e+00  -1.08864975e+00   9.87806737e-01\n",
      "   -7.17578232e-02  -1.00077968e-02  -1.37798876e-01   7.43504837e-02\n",
      "    9.69300643e-02  -3.42407852e-01  -1.41701567e+00   2.69931048e-01\n",
      "   -2.31277958e-01  -3.25431824e-02  -8.72800446e+00   4.10575867e-02\n",
      "    9.10638928e-01   1.29475966e-01  -1.22843273e-01   3.72669250e-01\n",
      "   -1.00400031e+00   9.01509345e-01   3.60197783e-01   1.22121859e+00\n",
      "    3.82787848e+00   1.49658382e+00   5.18991470e+00  -1.00000000e+00\n",
      "    6.08808517e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    7.61166334e-01]\n",
      " [  6.42890930e-01  -3.92386889e+00   4.40452576e-01   9.95236456e-01\n",
      "    8.00956637e-02  -4.45379596e-03   5.54006509e-02  -3.75043362e-01\n",
      "    1.35942809e-02   1.30683899e-01   5.21613240e-01   2.36763149e-01\n",
      "    1.47232330e+00  -3.60759735e-01  -9.29067230e+00   1.73431206e+00\n",
      "    9.47828531e-01  -2.46146739e-01   7.36383870e-02   1.88706815e-01\n",
      "   -8.62229645e-01  -4.16831255e-01  -7.09902883e-01  -2.26120377e+00\n",
      "    3.12963843e+00   4.82889938e+00  -7.97621155e+00  -1.00000000e+00\n",
      "    6.16491318e-01   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    3.54782343e-02]\n",
      " [ -5.38055420e-01  -3.79840469e+00  -1.13644421e+00   9.87330019e-01\n",
      "   -6.63719922e-02  -9.66381188e-03  -1.43808201e-01   1.08115263e-01\n",
      "    9.74147022e-02  -3.29750866e-01  -1.37106562e+00   2.90572196e-01\n",
      "   -3.63690168e-01  -1.22566223e-02  -8.73863125e+00  -4.11890149e-02\n",
      "    9.13951278e-01   1.22961961e-01  -1.14173263e-01   3.69510353e-01\n",
      "   -9.88794446e-01   8.66150260e-01   3.60646695e-01   1.21428561e+00\n",
      "    3.79231334e+00   1.36923528e+00   4.78367615e+00  -1.00000000e+00\n",
      "    6.41220951e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    4.68097657e-01]\n",
      " [  7.50711322e-01  -3.90983772e+00  -3.91654313e-01   9.94364738e-01\n",
      "    9.36696902e-02   4.65683499e-03  -4.94278111e-02  -1.61406651e-01\n",
      "   -4.58989851e-03   4.45492752e-02   1.76898226e-01  -3.20447870e-02\n",
      "    6.37618959e-01  -4.45068479e-02  -8.84424973e+00   1.09423578e+00\n",
      "    8.99269760e-01  -1.87761560e-01   1.57540679e-01   3.62271249e-01\n",
      "   -1.16504335e+00  -4.36710298e-01  -2.13837519e-01  -1.47124541e+00\n",
      "    3.38111091e+00   3.79285312e+00   3.43500853e+00  -1.00000000e+00\n",
      "   -7.22500658e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -4.35696244e-01]\n",
      " [  5.99405289e-01  -3.94198871e+00   3.23408127e-01   9.96374786e-01\n",
      "    7.47000426e-02  -3.03839031e-03   4.05973159e-02  -3.20924193e-01\n",
      "    1.24909617e-02   1.66331008e-01   6.63374126e-01   1.93144873e-01\n",
      "    1.26542985e+00  -3.44608307e-01  -9.38060284e+00   1.51903152e+00\n",
      "    9.53343034e-01  -2.30109215e-01   6.81497082e-02   1.83146209e-01\n",
      "   -8.55096161e-01  -4.28850740e-01  -7.38241017e-01  -2.18915844e+00\n",
      "    2.98547816e+00   4.54885721e+00  -7.98792076e+00  -1.00000000e+00\n",
      "   -4.39449310e-01   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -6.28730536e-01]\n",
      " [  9.74300385e-01  -3.84320593e+00   5.37310004e-01   9.90150332e-01\n",
      "    1.21803246e-01  -8.41986667e-03   6.85235783e-02  -5.25790930e-01\n",
      "    2.36227196e-02   1.84154227e-01   7.24797606e-01   4.38039452e-01\n",
      "    2.01322746e+00  -1.40136719e-01  -8.88641548e+00   1.91280138e+00\n",
      "    9.19755936e-01  -3.05943012e-01   1.18080355e-01   2.15650097e-01\n",
      "   -1.03732216e+00  -6.25721633e-01  -7.90394723e-01  -2.21004343e+00\n",
      "    4.33736563e+00   5.24895954e+00  -7.51714325e+00  -1.00000000e+00\n",
      "   -2.73725033e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    9.74787235e-01]\n",
      " [ -1.34542465e-01  -3.99316812e+00  -1.91928864e-01   9.99574482e-01\n",
      "   -1.67318489e-02  -4.00120654e-04  -2.38887928e-02  -8.98820460e-02\n",
      "    1.33964082e-03  -2.69760676e-02  -1.08518668e-01  -1.43408086e-02\n",
      "    3.60863149e-01  -2.60675430e-01  -9.73144150e+00   8.79491806e-01\n",
      "    9.78331625e-01  -5.74528659e-03  -6.45787688e-03   2.06863835e-01\n",
      "   -6.22684240e-01  -1.99382808e-02  -1.16695248e-01  -7.02376425e-01\n",
      "    1.06967509e+00   3.17981100e+00  -5.00515366e+00  -1.00000000e+00\n",
      "    6.24086761e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    5.67709208e-02]\n",
      " [ -2.86813736e-01  -3.97865629e+00  -2.98320562e-01   9.98670042e-01\n",
      "   -3.56711410e-02  -1.32937485e-03  -3.72013561e-02  -4.61908504e-02\n",
      "    9.23419371e-03  -1.22176163e-01  -4.91379648e-01   1.98843759e-02\n",
      "    1.87277630e-01  -2.25244522e-01  -9.63479805e+00   8.15767705e-01\n",
      "    9.71835315e-01   4.05114666e-02  -2.85358019e-02   2.30392501e-01\n",
      "   -7.10493505e-01   1.61624506e-01   9.92132723e-02  -1.49753451e-01\n",
      "    1.44491827e+00   3.10226488e+00  -3.55142021e+00  -1.00000000e+00\n",
      "    7.16850185e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    3.16133469e-01]\n",
      " [  1.57484055e-01  -3.96665287e+00  -4.92172241e-01   9.97923076e-01\n",
      "    1.95543002e-02   1.20268133e-03  -6.13663234e-02  -5.82804121e-02\n",
      "   -4.25026100e-03   3.40836123e-02   1.37975067e-01  -2.36951262e-02\n",
      "    2.32972145e-01  -4.95014191e-02  -9.41612911e+00   8.06564331e-01\n",
      "    9.54736590e-01  -4.80279215e-02   2.81840805e-02   2.92193562e-01\n",
      "   -7.89995253e-01  -1.41708836e-01  -1.36412069e-01  -6.08129799e-01\n",
      "    1.93133998e+00   3.27469301e+00   4.95336723e+00  -1.00000000e+00\n",
      "   -6.28205109e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    4.18611974e-01]\n",
      " [  8.43000412e-01  -3.90981221e+00  -8.22792053e-02   9.94371414e-01\n",
      "    1.05432570e-01   1.10615999e-03  -1.04089566e-02  -2.41883546e-01\n",
      "   -2.70294445e-03   1.04404293e-01   4.10692573e-01   6.25712499e-02\n",
      "    9.53111231e-01  -7.97443390e-02  -8.92587662e+00   1.35893631e+00\n",
      "    9.06658947e-01  -2.30044991e-01   1.58356890e-01   3.16183478e-01\n",
      "   -1.19483840e+00  -6.17955148e-01  -3.83919984e-01  -1.84673429e+00\n",
      "    3.58384800e+00   4.05114508e+00   2.03582573e+00  -1.00000000e+00\n",
      "   -7.73662949e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -7.18573332e-01]\n",
      " [  1.94000244e-01  -3.96209097e+00  -5.15495300e-01   9.97638404e-01\n",
      "    2.40864363e-02   1.55264873e-03  -6.43055886e-02  -5.01252711e-02\n",
      "   -2.24184850e-03   1.71738323e-02   6.95445687e-02  -2.27441285e-02\n",
      "    2.00010762e-01  -4.02565002e-02  -9.37570572e+00   8.01868439e-01\n",
      "    9.51685846e-01  -5.54726310e-02   3.46101411e-02   3.00031692e-01\n",
      "   -8.17051649e-01  -1.23207137e-01  -1.11702360e-01  -5.92445552e-01\n",
      "    2.03897524e+00   3.27825832e+00   5.26826477e+00  -1.00000000e+00\n",
      "   -6.02041245e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    1.95290446e-01]\n",
      " [  4.07489777e-01  -3.95032668e+00  -4.80594635e-01   9.96900558e-01\n",
      "    5.06561026e-02   3.05563444e-03  -6.01166487e-02  -9.64753330e-02\n",
      "   -1.09616863e-02   8.90610591e-02   3.58934492e-01  -1.14816241e-02\n",
      "    3.87402385e-01   2.40173340e-02  -9.27710342e+00   8.76430511e-01\n",
      "    9.42880750e-01  -9.85831097e-02   6.97412193e-02   3.10473025e-01\n",
      "   -8.60480070e-01  -2.91781694e-01  -2.00304970e-01  -8.12955678e-01\n",
      "    2.32350016e+00   3.44764304e+00   4.09856033e+00  -1.00000000e+00\n",
      "   -6.87035751e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    2.29530931e-01]\n",
      " [ -6.57832921e-01  -3.80784321e+00  -1.03739548e+00   9.87911701e-01\n",
      "   -8.13780725e-02  -1.08266724e-02  -1.31495357e-01   1.51270166e-01\n",
      "    9.06796381e-02  -3.38476449e-01  -1.39128506e+00   3.68729085e-01\n",
      "   -5.23001492e-01  -1.27979755e-01  -8.84402275e+00  -1.86805725e-02\n",
      "    9.20414209e-01   1.37675375e-01  -1.26477182e-01   3.43346566e-01\n",
      "   -9.96160269e-01   8.58305037e-01   3.72604907e-01   1.05026901e+00\n",
      "    3.79464841e+00   1.13210678e+00   6.18335390e+00  -1.00000000e+00\n",
      "    5.07603455e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -8.59579086e-01]\n",
      " [  7.49100089e-01  -3.90766978e+00   4.17068481e-01   9.94219244e-01\n",
      "    9.34511349e-02  -4.93971864e-03   5.26365414e-02  -4.06324834e-01\n",
      "    1.90785155e-02   1.94929376e-01   7.74149239e-01   2.99555004e-01\n",
      "    1.58437371e+00  -2.90630639e-01  -9.14663887e+00   1.78919220e+00\n",
      "    9.35480952e-01  -2.62460917e-01   9.67346206e-02   2.15944663e-01\n",
      "   -1.05371594e+00  -5.80841303e-01  -7.57024646e-01  -2.18122029e+00\n",
      "    3.87834191e+00   5.11007547e+00  -7.39065742e+00  -1.00000000e+00\n",
      "   -3.06238174e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -5.87672472e-01]\n",
      " [ -5.56682587e-01  -3.86518002e+00  -8.69679272e-01   9.91547465e-01\n",
      "   -6.89850152e-02  -7.62779266e-03  -1.09620444e-01  -3.91135029e-02\n",
      "    5.81645854e-02  -2.58784324e-01  -1.05677271e+00   1.04445994e-01\n",
      "    1.83199435e-01  -6.50558472e-02  -9.00490856e+00   2.59711146e-01\n",
      "    9.28632259e-01   1.25253186e-01  -1.04671232e-01   3.33163172e-01\n",
      "   -7.96046615e-01   6.28134191e-01   3.25101852e-01   8.87722671e-01\n",
      "    2.67512393e+00   2.19600391e+00  -2.85035706e+00  -1.00000000e+00\n",
      "    7.47498989e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -3.96821141e-01]\n",
      " [  7.77719498e-01  -3.88566852e+00   5.50228119e-01   9.92829919e-01\n",
      "    9.69536752e-02  -6.79021236e-03   6.95891529e-02  -4.63754535e-01\n",
      "    1.58226416e-02   1.20454311e-01   4.79397476e-01   3.30924928e-01\n",
      "    1.80223227e+00  -3.11725616e-01  -9.08933163e+00   1.94998550e+00\n",
      "    9.34267998e-01  -2.77723670e-01   9.22488496e-02   2.03722894e-01\n",
      "   -9.15747583e-01  -4.69456583e-01  -7.15769351e-01  -2.30971503e+00\n",
      "    3.61615443e+00   5.18044567e+00  -7.99019051e+00  -1.00000000e+00\n",
      "    3.96060944e-01   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    5.95939159e-01]\n",
      " [ -5.75523376e-02  -3.95606804e+00  -5.90030670e-01   9.97262776e-01\n",
      "   -7.13897496e-03  -5.27022057e-04  -7.35920593e-02  -9.56584048e-03\n",
      "    2.37284414e-02  -1.59616306e-01  -6.48672044e-01   1.45945488e-03\n",
      "    3.90920341e-02  -1.14547729e-01  -9.37688065e+00   6.96849823e-01\n",
      "    9.53330219e-01  -3.69798043e-03  -3.96691915e-03   3.01880866e-01\n",
      "   -7.90702820e-01   2.15680018e-01   1.11096829e-01  -1.82962671e-01\n",
      "    1.98013401e+00   2.98893213e+00   7.51549911e+00  -1.00000000e+00\n",
      "   -2.74176407e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -5.23933172e-01]\n",
      " [  7.52265275e-01  -3.91674733e+00  -3.11779022e-01   9.94796813e-01\n",
      "    9.39071998e-02   3.71758733e-03  -3.93318981e-02  -2.81282872e-01\n",
      "   -1.41070951e-02   1.66019514e-01   6.58391178e-01   2.86391973e-02\n",
      "    1.11792970e+00   2.50082016e-02  -8.97044182e+00   1.14919662e+00\n",
      "    9.12175834e-01  -1.82446554e-01   1.43097118e-01   3.37893307e-01\n",
      "   -9.95627046e-01  -6.22802138e-01  -3.69824529e-01  -1.57672274e+00\n",
      "    3.20148182e+00   3.98498368e+00   4.53251481e-01  -1.00000000e+00\n",
      "   -7.98714828e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    5.68796635e-01]\n",
      " [  9.68111038e-01  -3.87382460e+00   2.50453949e-01   9.92103279e-01\n",
      "    1.21244609e-01  -3.88702122e-03   3.18701230e-02  -4.46264148e-01\n",
      "    9.79400240e-03   1.77273780e-01   6.93338096e-01   2.65394151e-01\n",
      "    1.73072839e+00  -7.21721649e-02  -8.89840126e+00   1.62506104e+00\n",
      "    9.11412537e-01  -2.79220223e-01   1.49430275e-01   2.62742937e-01\n",
      "   -1.06175804e+00  -7.22282350e-01  -6.56326115e-01  -2.15159678e+00\n",
      "    3.97721958e+00   4.59473228e+00  -2.90072823e+00  -1.00000000e+00\n",
      "   -7.45558548e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    1.31489396e-01]],  actions: [[ 0.03735632 -0.11652847 -0.15901251  0.20029056]\n",
      " [ 0.05774632 -0.14135887 -0.15585765  0.19724475]\n",
      " [-0.37213928 -0.02026295 -0.21223614 -0.21379   ]\n",
      " [ 0.07311355 -0.13246322 -0.15892346  0.19288933]\n",
      " [-0.158447   -0.08127262 -0.11280869 -0.11756772]\n",
      " [-0.38817725 -0.00272885 -0.23756374 -0.22019947]\n",
      " [-0.43787253  0.0292224  -0.20222527 -0.24619028]\n",
      " [-0.1872738  -0.0547985  -0.16797073 -0.03532908]\n",
      " [-0.15720151 -0.05421057 -0.1662685   0.0391769 ]\n",
      " [-0.14627604 -0.04565879 -0.15198761 -0.09155128]\n",
      " [-0.18394224 -0.08647956 -0.12155112 -0.15825844]\n",
      " [-0.13742977 -0.04959954 -0.14926212 -0.07834799]\n",
      " [-0.15778215 -0.05452769 -0.14301662 -0.11625358]\n",
      " [ 0.12071728 -0.1335801  -0.15787642  0.1955235 ]\n",
      " [-0.42094767  0.04514692 -0.23389852 -0.23894501]\n",
      " [-0.12342946 -0.07064486 -0.12292824  0.09215435]\n",
      " [-0.38298321 -0.01213654 -0.19310696 -0.21983784]\n",
      " [-0.07636566 -0.07620612 -0.15252845  0.04254968]\n",
      " [-0.24926628 -0.06503633 -0.14432196 -0.20072292]\n",
      " [-0.3450186  -0.06661096 -0.18730275 -0.23730572]],  rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  next_states: [[ -7.34920502e-01  -3.81920052e+00  -9.38949585e-01   9.88625348e-01\n",
      "   -9.11338776e-02  -1.09842774e-02  -1.19138658e-01  -4.43814285e-02\n",
      "    8.36614072e-02  -3.40576917e-01  -1.38786030e+00   1.99010864e-01\n",
      "    2.29741752e-01  -9.14955139e-02  -8.69087410e+00   2.47077942e-01\n",
      "    9.00316119e-01   1.56256735e-01  -1.54235035e-01   3.75800878e-01\n",
      "   -9.25483882e-01   9.39778388e-01   3.57390493e-01   1.27438080e+00\n",
      "    3.52431583e+00   1.62364554e+00   7.73819733e+00  -1.00000000e+00\n",
      "    2.02984619e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -1.29508138e-01]\n",
      " [ -6.97269440e-01  -3.78817582e+00  -1.08300781e+00   9.86658990e-01\n",
      "   -8.62189233e-02  -1.20232403e-02  -1.37571350e-01  -3.55809443e-02\n",
      "    1.02455117e-01  -3.59095842e-01  -1.48035192e+00   2.01122686e-01\n",
      "    2.04063550e-01  -2.89688110e-02  -8.51181126e+00   1.22203827e-01\n",
      "    8.87920916e-01   1.50852650e-01  -1.58574015e-01   4.04591352e-01\n",
      "   -9.77009237e-01   1.01642120e+00   3.18474472e-01   1.34403026e+00\n",
      "    3.78867650e+00   1.43915129e+00   5.50595474e+00  -1.00000000e+00\n",
      "    5.80383301e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    7.61166334e-01]\n",
      " [  6.72111511e-01  -3.90198064e+00   5.72732925e-01   9.93857086e-01\n",
      "    8.36716518e-02  -6.07384648e-03   7.21835196e-02  -4.44513559e-01\n",
      "    1.03431614e-02   7.55553767e-02   3.02334487e-01   2.89379358e-01\n",
      "    1.73910439e+00  -4.69387054e-01  -9.09686756e+00   2.07811165e+00\n",
      "    9.32253718e-01  -2.71593511e-01   9.37537849e-02   2.19887048e-01\n",
      "   -9.03209865e-01  -4.38483059e-01  -6.47889018e-01  -2.37623215e+00\n",
      "    3.45778084e+00   5.08737850e+00  -7.97465897e+00  -1.00000000e+00\n",
      "    6.36255264e-01   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    3.54782343e-02]\n",
      " [ -6.50909424e-01  -3.77900481e+00  -1.14241743e+00   9.86083746e-01\n",
      "   -8.03602040e-02  -1.18214246e-02  -1.45056680e-01   3.23879067e-03\n",
      "    1.04334176e-01  -3.46862823e-01  -1.43858778e+00   2.19561011e-01\n",
      "    5.26099503e-02  -7.00187683e-03  -8.52317047e+00   2.65890956e-02\n",
      "    8.92240405e-01   1.44273326e-01  -1.48373187e-01   4.01344776e-01\n",
      "   -9.61156845e-01   9.79093611e-01   3.22668165e-01   1.33996952e+00\n",
      "    3.76202869e+00   1.31445491e+00   4.99063301e+00  -1.00000000e+00\n",
      "    6.25248623e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    4.68097657e-01]\n",
      " [  7.51292169e-01  -3.91626573e+00  -3.19707215e-01   9.94768322e-01\n",
      "    9.37816128e-02   3.80164920e-03  -4.03314419e-02  -2.59887516e-01\n",
      "    1.73198420e-03  -2.00140160e-02  -7.93284029e-02  -1.03553340e-01\n",
      "    1.02113962e+00  -1.29133582e-01  -8.67446327e+00   1.34250414e+00\n",
      "    8.78821433e-01  -1.93292111e-01   1.76188052e-01   3.99084985e-01\n",
      "   -1.02821672e+00  -3.46114546e-01  -1.96806625e-01  -1.64584291e+00\n",
      "    3.09335113e+00   3.89999604e+00   3.65315342e+00  -1.00000000e+00\n",
      "   -7.11719513e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -4.35696244e-01]\n",
      " [  6.42457962e-01  -3.92390990e+00   4.40830231e-01   9.95238245e-01\n",
      "    8.00416693e-02  -4.45456197e-03   5.54476157e-02  -3.98224801e-01\n",
      "    1.27778761e-02   1.23289786e-01   4.92079049e-01   2.41030052e-01\n",
      "    1.56443024e+00  -4.41370010e-01  -9.19418621e+00   1.84086800e+00\n",
      "    9.38008606e-01  -2.57462949e-01   8.86104330e-02   2.14478061e-01\n",
      "   -9.06035185e-01  -4.70121264e-01  -6.91962838e-01  -2.30064797e+00\n",
      "    3.38667297e+00   4.85445404e+00  -7.96094513e+00  -1.00000000e+00\n",
      "   -7.89529800e-01   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -6.28730536e-01]\n",
      " [  1.01632309e+00  -3.80311012e+00   7.16752827e-01   9.87590373e-01\n",
      "    1.26901239e-01  -1.17857531e-02   9.17739645e-02  -6.17541492e-01\n",
      "    1.98653489e-02   1.13762066e-01   4.49449360e-01   5.28110862e-01\n",
      "    2.34755349e+00  -2.38479614e-01  -8.61617565e+00   2.30622935e+00\n",
      "    8.96856487e-01  -3.32370698e-01   1.48992673e-01   2.50957102e-01\n",
      "   -1.12362683e+00  -6.62124455e-01  -6.99544430e-01  -2.32209301e+00\n",
      "    4.69857311e+00   5.51542330e+00  -7.68587875e+00  -1.00000000e+00\n",
      "   -2.21974301e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    9.74787235e-01]\n",
      " [ -1.43003464e-01  -3.99472117e+00  -1.48773193e-01   9.99670327e-01\n",
      "   -1.77862793e-02  -3.29858594e-04  -1.85160711e-02  -1.58486694e-01\n",
      "    1.05177274e-03  -2.61228643e-02  -1.05034858e-01  -2.19345391e-02\n",
      "    6.36360466e-01  -3.05480957e-01  -9.67391491e+00   1.08164406e+00\n",
      "    9.73194003e-01  -1.10055478e-02  -6.18258305e-03   2.29639456e-01\n",
      "   -5.64165115e-01  -3.47804949e-02  -1.28468007e-01  -7.65812814e-01\n",
      "    1.06921530e+00   3.36898661e+00  -4.98038101e+00  -1.00000000e+00\n",
      "    6.26065445e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    5.67709208e-02]\n",
      " [ -3.27556610e-01  -3.97769403e+00  -2.67870575e-01   9.98609424e-01\n",
      "   -4.07517627e-02  -1.36537454e-03  -3.34192701e-02  -1.19905166e-01\n",
      "    9.01105069e-03  -1.29733399e-01  -5.21157980e-01   7.85189774e-03\n",
      "    4.82221812e-01  -2.55769730e-01  -9.55563354e+00   1.00545108e+00\n",
      "    9.64946032e-01   4.52215150e-02  -3.54507267e-02   2.56081134e-01\n",
      "   -6.44689083e-01   1.79348215e-01   1.03021026e-01  -1.26541868e-01\n",
      "    1.43971705e+00   3.26282406e+00  -3.39235687e+00  -1.00000000e+00\n",
      "    7.24513102e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    3.16133469e-01]\n",
      " [  1.66627884e-01  -3.97074509e+00  -4.54685211e-01   9.98177171e-01\n",
      "    2.06958801e-02   1.17555889e-03  -5.66800795e-02  -1.47644237e-01\n",
      "   -2.92619132e-03   2.50146091e-02   1.01159818e-01  -6.47846535e-02\n",
      "    5.89499295e-01  -8.16764832e-02  -9.31821156e+00   1.00885010e+00\n",
      "    9.45355237e-01  -5.43415137e-02   3.35877202e-02   3.19722444e-01\n",
      "   -6.94419324e-01  -1.51733041e-01  -1.47227079e-01  -7.45907068e-01\n",
      "    1.79162395e+00   3.41776490e+00   4.76768875e+00  -1.00000000e+00\n",
      "   -6.42410660e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    4.18611974e-01]\n",
      " [  8.59825134e-01  -3.90703678e+00   1.47781372e-02   9.94196057e-01\n",
      "    1.07567340e-01  -2.01496077e-04   1.87045790e-03  -3.39144439e-01\n",
      "   -8.93996985e-05   2.87348572e-02   1.12856425e-01   2.05057301e-02\n",
      "    1.33205652e+00  -1.76126480e-01  -8.73571682e+00   1.63462448e+00\n",
      "    8.84334266e-01  -2.42190838e-01   1.84960842e-01   3.53675216e-01\n",
      "   -1.08826280e+00  -5.43900967e-01  -3.34346533e-01  -2.04765105e+00\n",
      "    3.41454911e+00   4.13607168e+00   2.42121315e+00  -1.00000000e+00\n",
      "   -7.62480927e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -7.18573332e-01]\n",
      " [  1.95716858e-01  -3.96634531e+00  -4.80911255e-01   9.97902870e-01\n",
      "    2.43062992e-02   1.46082649e-03  -5.99756986e-02  -1.38118267e-01\n",
      "    1.53689063e-04  -1.24455791e-03  -5.03516290e-03  -6.82159066e-02\n",
      "    5.50366163e-01  -7.51457214e-02  -9.27256489e+00   1.00244904e+00\n",
      "    9.41777945e-01  -6.00516871e-02   3.93712185e-02   3.28478277e-01\n",
      "   -7.20165133e-01  -1.12766422e-01  -1.13839336e-01  -7.10411131e-01\n",
      "    1.88464355e+00   3.41475081e+00   5.18569565e+00  -1.00000000e+00\n",
      "   -6.09168243e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    1.95290446e-01]\n",
      " [  4.30080414e-01  -3.95386481e+00  -4.29069519e-01   9.97120500e-01\n",
      "    5.34919240e-02   2.88065104e-03  -5.36751263e-02  -1.93844795e-01\n",
      "   -6.71915524e-03   6.00141548e-02   2.41403326e-01  -6.06906265e-02\n",
      "    7.72934079e-01  -1.24893188e-02  -9.15910912e+00   1.09575462e+00\n",
      "    9.30937469e-01  -1.07132033e-01   8.10616389e-02   3.39569241e-01\n",
      "   -7.53647983e-01  -2.75556952e-01  -1.95629820e-01  -9.80526030e-01\n",
      "    2.14160824e+00   3.58527350e+00   3.98794174e+00  -1.00000000e+00\n",
      "   -6.93515205e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    2.29530931e-01]\n",
      " [ -7.69574165e-01  -3.78078938e+00  -1.05971146e+00   9.86181140e-01\n",
      "   -9.52743366e-02  -1.30311549e-02  -1.34906799e-01   6.50589764e-02\n",
      "    9.52181593e-02  -3.43070716e-01  -1.40674937e+00   3.23176622e-01\n",
      "   -1.77075461e-01  -1.31338507e-01  -8.62124252e+00   2.57453918e-02\n",
      "    8.98889422e-01   1.57496855e-01  -1.61021858e-01   3.75851929e-01\n",
      "   -9.82740998e-01   9.49409962e-01   3.29101413e-01   1.16437197e+00\n",
      "    3.84507227e+00   1.03887951e+00   5.86779308e+00  -1.00000000e+00\n",
      "    5.43774033e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -8.59579086e-01]\n",
      " [  7.99628258e-01  -3.87927318e+00   5.64281464e-01   9.92424250e-01\n",
      "    9.96989608e-02  -7.16905249e-03   7.14347437e-02  -5.05629241e-01\n",
      "    1.97381862e-02   1.46972522e-01   5.84585488e-01   3.81204367e-01\n",
      "    1.95995307e+00  -3.81563962e-01  -8.90457726e+00   2.15649605e+00\n",
      "    9.14762557e-01  -2.89336383e-01   1.25050262e-01   2.52698243e-01\n",
      "   -1.11086762e+00  -6.30564153e-01  -6.87808573e-01  -2.26217794e+00\n",
      "    4.32583761e+00   5.36931849e+00  -7.25883150e+00  -1.00000000e+00\n",
      "   -3.36293983e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -5.87672472e-01]\n",
      " [ -6.41704559e-01  -3.86063528e+00  -8.30610633e-01   9.91254508e-01\n",
      "   -7.96249583e-02  -8.43045674e-03  -1.04897074e-01  -1.56123623e-01\n",
      "    5.73499463e-02  -2.63805240e-01  -1.07228625e+00   3.19279507e-02\n",
      "    6.41534984e-01  -6.35490417e-02  -8.86568165e+00   4.03878570e-01\n",
      "    9.13725853e-01   1.42367601e-01  -1.28683060e-01   3.58158171e-01\n",
      "   -6.97795272e-01   6.75197959e-01   3.09289396e-01   9.54004169e-01\n",
      "    2.42866874e+00   2.31697631e+00  -3.05631638e+00  -1.00000000e+00\n",
      "    7.39316750e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -3.96821141e-01]\n",
      " [  8.01290512e-01  -3.85517502e+00   7.09354401e-01   9.90897000e-01\n",
      "    9.97567400e-02  -9.05220862e-03   8.99437889e-02  -5.36556065e-01\n",
      "    9.44681652e-03   5.49519621e-02   2.19706178e-01   4.02024597e-01\n",
      "    2.07611895e+00  -4.26120758e-01  -8.86590099e+00   2.32698441e+00\n",
      "    9.16396201e-01  -3.02339077e-01   1.15072206e-01   2.35727608e-01\n",
      "   -9.51627910e-01  -4.78581190e-01  -6.39045238e-01  -2.42415380e+00\n",
      "    3.90279531e+00   5.40955162e+00  -7.96680260e+00  -1.00000000e+00\n",
      "    7.28054047e-01   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    5.95939159e-01]\n",
      " [ -1.17809296e-01  -3.95760036e+00  -5.70556641e-01   9.97356772e-01\n",
      "   -1.46167316e-02  -1.04479480e-03  -7.11673275e-02  -8.60657915e-02\n",
      "    2.90375613e-02  -1.99899688e-01  -8.11778307e-01  -2.94172559e-02\n",
      "    3.45233828e-01  -1.57558441e-01  -9.27352810e+00   8.71723175e-01\n",
      "    9.43948388e-01   4.08791471e-03  -1.26853585e-02   3.29824150e-01\n",
      "   -7.06704617e-01   2.97950923e-01   1.29795671e-01  -1.48307517e-01\n",
      "    1.88012743e+00   3.07990050e+00   7.61073685e+00  -1.00000000e+00\n",
      "   -2.46509171e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   -5.23933172e-01]\n",
      " [  7.95100033e-01  -3.91594982e+00  -1.94179535e-01   9.94746864e-01\n",
      "    9.93544608e-02   2.45445548e-03  -2.45242510e-02  -4.16359037e-01\n",
      "   -6.97089685e-03   1.20636761e-01   4.76413995e-01   1.07560452e-04\n",
      "    1.64427471e+00  -4.46321368e-02  -8.80276966e+00   1.43583298e+00\n",
      "    8.92672062e-01  -1.98493302e-01   1.67318165e-01   3.68431419e-01\n",
      "   -8.63929927e-01  -6.14193559e-01  -3.65918189e-01  -1.85265815e+00\n",
      "    3.01449680e+00   4.15009069e+00   1.35809347e-01  -1.00000000e+00\n",
      "   -7.99884796e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    5.68796635e-01]\n",
      " [  1.00637054e+00  -3.85050726e+00   4.10762787e-01   9.90621805e-01\n",
      "    1.26002520e-01  -6.66190917e-03   5.24164326e-02  -5.51940382e-01\n",
      "    9.32052545e-03   9.82434303e-02   3.84286344e-01   3.01167756e-01\n",
      "    2.13038325e+00  -1.73557281e-01  -8.66427326e+00   1.97065735e+00\n",
      "    8.87714863e-01  -3.01986277e-01   1.80996388e-01   2.96659768e-01\n",
      "   -1.05860412e+00  -7.16054082e-01  -5.91638863e-01  -2.38022804e+00\n",
      "    4.13391972e+00   4.81299257e+00  -2.96904182e+00  -1.00000000e+00\n",
      "   -7.42864609e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    1.31489396e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(replay_buffer[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
